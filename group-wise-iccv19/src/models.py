# AUTOGENERATED! DO NOT EDIT! File to edit: 01_models.ipynb (unless otherwise specified).

__all__ = ['SIR', 'CARU', 'MFF', 'Model']

# Cell
import torch
import torchvision.models as models
import torch.nn as nn
from torchvision import datasets, transforms as T
from torch.nn.functional import max_pool2d, interpolate
import imageio
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
import torch.optim as optim
import matplotlib.pyplot as plt


# Cell
class SIR(nn.Module):
    """
    Single Image Representation (Section 3.2)
    Takes input Xn
    input_shape: (tuple) Shape of Xn i.e. (batch_size, 512,7,7)
    """

    def __init__(self, input_shape):
        super(SIR, self).__init__()

        Xn_shape = input_shape[1]
        self.conv1 = nn.Conv2d(Xn_shape, Xn_shape//2, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()

        self.conv2 = nn.Conv2d(Xn_shape//2, Xn_shape//4, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()

        self.conv3 = nn.Conv2d(Xn_shape//4, Xn_shape//8, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()


    def forward(self, Xn):
        res = self.relu1(self.conv1(Xn))
        res = self.relu2(self.conv2(res))
        Sn = self.relu3(self.conv3(res))

        return Sn

# Cell
class CARU(nn.Module):
    """ Recursive model it implements two gates reset and update (Section 3.3)"""
    def __init__(self, input_shape):
        super(CARU, self).__init__()
        self.b, self.c, self.w, self.h = input_shape
        self.W_1_s = nn.Parameter(torch.rand(self.b, 64, 1, requires_grad=True)) # [PAPER] 64 is arbitrary: no info about dimensionality
        self.W_2_s = nn.Parameter(torch.rand(self.b, 1, 64, requires_grad=True))
        self.W_c = nn.Parameter(torch.rand(self.b, 1, 1, requires_grad=True))
        self.W_d = nn.Parameter(torch.rand(self.b,1, 2, requires_grad=True))


    def reset_gate(self, Xn, Gn_1):
        gx = torch.cat([Gn_1,Xn], dim=1).reshape(self.b, 2, -1)
        tmp = torch.bmm(self.W_d, gx).reshape(self.b, self.c , self.w, self.h)
        X_tilde_n = Xn * torch.sigmoid(tmp)
        return X_tilde_n


    def update_gate(self, Xn, Gn_1):

        Zn = Gn_1 - Xn

        # Spatial attention model
        relu = nn.ReLU()
        cross_channel_avgpool = nn.AvgPool3d((self.c, 1, 1))
        Zhwn = cross_channel_avgpool(Zn).reshape(self.b, 1, -1)
        res = relu(torch.bmm(self.W_1_s, Zhwn))
        Zs = torch.bmm(self.W_2_s, res).reshape(self.b, 1, self.w, self.h)

        # Channel attention model
        cross_spatial_avgpool = nn.AvgPool3d((1, self.w, self.h))
        Zcn = cross_spatial_avgpool(Zn).reshape(self.b, self.c, 1)
        Zc = torch.bmm(Zcn, self.W_c).reshape(self.b, self.c, 1, 1)

        Z = torch.sigmoid(Zs * Zc)

        return Z


    def forward(self, Xn, Gn_1):
        Z = self.update_gate(Xn, Gn_1)
        X_tilde_n = self.reset_gate(Xn, Gn_1)
        Gn = Z * Gn_1 + (1-Z) * X_tilde_n
        return Gn

# Cell
class MFF(nn.Module):
    """ Fused representation (Section 3.4) this section was difficult to implement
    the paper is not so clear about some choices """

    def __init__(self, input_shape):
        super(MFF, self).__init__()

        self.input_shape = input_shape

        self.conv_v3 = nn.Conv2d(768, 128, kernel_size=1)
        self.conv_v4 = nn.Conv2d(1024, 128, kernel_size=1)
        self.conv_v5 = nn.Conv2d(576, 128, kernel_size=1)

        self.convT_v5 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=4, output_padding=1)
        self.convT_v4 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)

        self.conv_vhat = nn.Conv2d(128, 128, kernel_size=3, padding=1)

        # Dunno how to set these parameters...I just match the output size...
        self.convT_vhat = nn.ConvTranspose2d(128, 1, kernel_size=14, padding=3, stride=7, dilation=3, output_padding=1)

    def forward(self, Sn, Gn, p3, p4):

        Gn_up3 = interpolate(Gn, p3.shape[2:])
        v3 = torch.cat((p3, Gn_up3), dim=1)
        v3 = self.conv_v3(v3)

        Gn_up4 = interpolate(Gn, p4.shape[2:])
        v4 = torch.cat((p4, Gn_up4), dim=1)
        v4 = self.conv_v4(v4)

        v5 = torch.cat((Sn, Gn), dim=1)
        v5 = self.conv_v5(v5)

        v5 = self.convT_v5(v5)
        v4 = self.convT_v4(v4)

        vhat = v5 + v4 +v3
        vhat = self.conv_vhat(vhat)
        vhat = self.convT_vhat(vhat)

        Mn = torch.sigmoid(vhat)

        return Mn

# Cell
class Model(nn.Module):
    """ The overall model, i parametrized with input_shape, but it only works for (1,3,224,224) images """
    def __init__(self, input_shape):
        super(Model, self).__init__()

        self.input_shape = input_shape

        vgg19_original = models.vgg19()
        self.vgg19 = nn.Sequential((*(list(vgg19_original.children())[:-2])))
        for param in self.vgg19.parameters():
            param.requires_grad = False

        self.pool3 = self.vgg19[0][:19]
        for param in self.pool3.parameters():
            param.requires_grad = False

        self.pool4 = self.vgg19[0][:28]
        for param in self.pool4.parameters():
            param.requires_grad = False

        # Unecessary
        with torch.no_grad():
            testinput = torch.rand(input_shape)
            self.Xn_shape = self.vgg19(testinput).shape

        self.sir = SIR(self.Xn_shape)
        self.caru = CARU(self.Xn_shape)
        self.mff = MFF(self.input_shape)


    def cuda(self):
        # Dunno if I call cuda on the module automatically triggers all the childs
        # I put it anyway
        self.sir = self.sir.cuda()
        self.caru = self.caru.cuda()
        self.mff = self.mff.cuda()
        self.vgg19 = self.vgg19.cuda()
        self.pool3 = self.pool3.cuda()
        self.pool4 = self.pool4.cuda()

        return self


    def forward(self, I):

        Sns = []
        for i, im in enumerate(I):
            Xn = self.vgg19(im)
            Sn = self.sir(Xn)
            Sns.append(Sn)
            if i == 0:
                self.Gn = self.caru(Xn, Xn)
            else:
                self.Gn = self.caru(Xn, self.Gn)

        Mns = []
        for im,Sn in zip(I,Sns):
            p3 = self.pool3(im)
            p4 = self.pool4(im)
            Mn = self.mff(Sn, self.Gn, p3, p4)
            Mns.append(Mn)

        return tuple(Mns)
